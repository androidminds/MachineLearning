{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b446423",
   "metadata": {},
   "source": [
    "# 一个二分类交叉熵的三层全连接神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb039dd",
   "metadata": {},
   "source": [
    "一个三层全连接网络包含输入层，隐藏层和输出层。隐藏层加sigmoid作为激活函数，输出层加sigmoid做为二分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "63765c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75396a38",
   "metadata": {},
   "source": [
    "sigmoid函数的公式是：\n",
    "$f(x) = \\displaystyle\\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "00965624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5a0fd",
   "metadata": {},
   "source": [
    "对sigmoid函数求导的公式是：$f'(x) = \\displaystyle\\frac{1}{1+e^{-x}} * (1 - \\frac{1}{1+e^{-x}}) = f(x) * (1 -f(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b2d91fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "  y = sigmoid(x)\n",
    "  return y * (1 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4451b0",
   "metadata": {},
   "source": [
    "这里使用交叉熵(cross entropy)作为损失函数，假设y为标签，$\\hat{y}$是预测结果，则有\\\n",
    "$\\displaystyle L(y，\\hat{y})=-(y*log\\hat{y} + (1-y)*log\\hat{y})$ \\\n",
    "对一批训练集的结果还需要做一下平均，最后的结果就是：\\\n",
    "$\\displaystyle\\varepsilon =\\frac{1}{m}\\sum_{i=1}^{m}L(y_{i}, \\hat{y_{i}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e523d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return - (y * np.log(y_hat)) + (1 - y) * np.log(1 - y_hat)\n",
    "\n",
    "def loss_func(y_hat, y) :\n",
    "    return cross_entropy(y_hat, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11de236",
   "metadata": {},
   "source": [
    "函数$L(y，\\hat{y})$对$\\hat{y}$求导的公式是:\n",
    "$\\displaystyle \\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "81f57cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_deriv(y_hat, y):\n",
    "    eps = 0.000000000001\n",
    "    return -np.divide(y, y_hat + eps) + np.divide(1 - y, 1 - y_hat + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ed70e",
   "metadata": {},
   "source": [
    "定义一个类SimpleNeuralNetwork，参数随机初始化为\\[0,1)区间的数\n",
    "\n",
    "假定输入为x，标签为y；中间层结果为h$\\hat{y}$，输出结果为$\\hat{y}$，每层神经元的计算公式为：\n",
    "$\\displaystyle w^{T}x+b$，那么一个三层神经网络的公式为：\\\n",
    "$\\displaystyle h = w_{1}^{T}x+b_{1}$  \\\n",
    "$\\displaystyle \\hat{h} = sigmoid(h)$ \\\n",
    "$\\displaystyle o = w_{2}^{T}\\hat{h}+b_{2}$   \\\n",
    "$\\hat{y} = sigmoid(o)$\n",
    "\n",
    "对于反向传播算法，就是计算损失函数对参数w和b的导数，再利用这些导数和学习率来更新参数。\\\n",
    "$\\displaystyle\\frac{\\partial L }{\\partial w_{2}} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial\\hat{y}}{\\partial o}\\frac{\\partial o}{\\partial w_{2}} = \\frac{crossentropy'(\\hat{y}, y) * sigmoid'(o))^{T}*\\hat{h} }{ m}$\\\n",
    "$\\displaystyle\\frac{\\partial L }{\\partial b_{2}} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial\\hat{y}}{\\partial o}\\frac{\\partial o}{\\partial b_{2}} = \\frac{\\sum(crossentropy'(\\hat{y}, y) * sigmoid'(o))}{m}$ \\\n",
    "我们这里使用的batch训练法，x包含了一批数据（每行代表一个数据）。计算w1'的矩阵运算会通过数据相加来消除维度，因此w1'的值都要除以m (这里m表示一批数据的大小), 计算b2'得到的结果包含了一批数据，因此要计算它们的平均值。后面w1'和b1'的计算都类似\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e4120c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input1, hidden, output):\n",
    "        self.w1 = np.random.random((hidden, input1))\n",
    "        self.w2 = np.random.random((output, hidden))\n",
    "        self.b1 = np.random.random((hidden, 1))\n",
    "        self.b2 = np.random.random((output, 1))\n",
    "    def feed_forward(self, x):\n",
    "        self.x = x\n",
    "        self.h = np.dot(x, self.w1.T) + self.b1.T\n",
    "        self.h_hat = sigmoid(self.h)\n",
    "        #\n",
    "        self.o = np.dot(self.h_hat, self.w2.T) + self.b2.T\n",
    "        self.y_hat = sigmoid(self.o)\n",
    "        return self.y_hat\n",
    "    def feed_backward(self, y, lr):\n",
    "        d_o = cross_entropy_deriv(self.y_hat, y) * sigmoid_deriv(self.o) \n",
    "        dw2 =np.dot(d_o.T, self.h_hat) / d_o.shape[0]\n",
    "        db2 = d_o.mean(axis = 0).reshape(-1, 1)\n",
    "        dh_hat = d_o * self.w2\n",
    "        #\n",
    "        d_h = dh_hat*sigmoid_deriv(self.h)\n",
    "        dw1 = np.dot(d_h.T, self.x) / d_h.shape[0]\n",
    "        db1 = d_h.mean(axis = 0).reshape(-1, 1)\n",
    "       \n",
    "        self.w1 -= lr * dw1\n",
    "        self.w2 -= lr * dw2\n",
    "        self.b1 -= lr * db1\n",
    "        self.b2 -= lr * db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a85c3",
   "metadata": {},
   "source": [
    "这里定义一个2x3x1的网络来用于训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a6b89500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, label):\n",
    "    model = SimpleNeuralNetwork(2, 3, 1)\n",
    "    learn_rate = 0.1\n",
    "    epochs = 10000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_hat = model.feed_forward(data)\n",
    "        loss = loss_func(y_hat, label)\n",
    "        model.feed_backward(label, learn_rate)\n",
    "\n",
    "        #print(\"Epoch %d loss:\", loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a57329",
   "metadata": {},
   "source": [
    "使用西瓜数据集来做测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7434a89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wm1: 0.266\n",
      "wm2: 0.885\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X = np.array([[0.607, 0.460],\n",
    "                  [0.774, 0.376],\n",
    "                  [0.634, 0.264],\n",
    "                  [0.608, 0.318],\n",
    "                  [0.556, 0.215],\n",
    "                  [0.403, 0.237],\n",
    "                  [0.481, 0.149],\n",
    "                  [0.437, 0.211],\n",
    "                  [0.666, 0.091],\n",
    "                  [0.243, 0.267],\n",
    "                  [0.245, 0.057],\n",
    "                  [0.343, 0.099],\n",
    "                  [0.639, 0.161],\n",
    "                  [0.657, 0.198],\n",
    "                  [0.360, 0.370],\n",
    "                  [0.593, 0.042],\n",
    "                  [0.719, 0.103],\n",
    "                  ])\n",
    "    y = np.array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "    model = train(X, y.reshape(-1,1))\n",
    "\n",
    "    wm1 = np.array([0.719, 0.103]).reshape(1, -1)\n",
    "    wm2 = np.array([0.607, 0.406]).reshape(1, -1)\n",
    "    print(f\"wm1: {np.squeeze(model.feed_forward(wm1)):.3f}\")\n",
    "    print(f\"wm2: {np.squeeze(model.feed_forward(wm2)):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d211126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1",
   "language": "python",
   "name": "pytorch2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
